{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Final_version_submit]_Project Code_Shahad Alghamdi.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMXLdyO+dJrirPPJIHP5XDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahad-UOL/Project/blob/main/%5BFinal_version_submit%5D_Project_Code_Shahad_Alghamdi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5eEB_RqOOQ2"
      },
      "source": [
        "# Run the below cell first"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tzUohrcetCR"
      },
      "source": [
        "pip install pyldavis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb9fxEQ_fCYv"
      },
      "source": [
        "# Download the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At84VcEWfV-Z"
      },
      "source": [
        "# This is the basic steps performed to merge both data and remove unessarly info\n",
        "\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsAYt2xQoEvi"
      },
      "source": [
        "def read_and_merge(file1, file2):\n",
        "  \n",
        "  #Read file and store in df\n",
        "  meta_data=[]\n",
        "  with gzip.open(file1) as f:\n",
        "      for l in f:\n",
        "          meta_data.append(json.loads(l.strip()))\n",
        "\n",
        "  df_meta_data = pd.DataFrame.from_dict(meta_data)\n",
        "\n",
        "  #Read file and store in df\n",
        "  review_data=[]\n",
        "  with gzip.open(file2) as f:\n",
        "      for l in f:\n",
        "          review_data.append(json.loads(l.strip()))\n",
        "\n",
        "  df_reviews = pd.DataFrame.from_dict(review_data)\n",
        "\n",
        "  #Merge dfs\n",
        "  df_merged = pd.merge(df_reviews, df_meta_data, on=['asin','asin'])\n",
        "  return df_merged\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU1ULJU4pRnm"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Arts_Crafts_and_Sewing.json.gz\n",
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Arts_Crafts_and_Sewing.json.gz\n",
        "\n",
        "df_Arts= read_and_merge(\"meta_Arts_Crafts_and_Sewing.json.gz\",\"Arts_Crafts_and_Sewing.json.gz\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1Eeam8sXgL"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Video_Games.json.gz\n",
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Video_Games.json.gz\n",
        "\n",
        "df_Video_Games= read_and_merge(\"meta_Video_Games.json.gz\",\"Video_Games.json.gz\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgBdIrxMt1Zv"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Grocery_and_Gourmet_Food.json.gz\n",
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Grocery_and_Gourmet_Food.json.gz\n",
        "\n",
        "df_Grocery= read_and_merge(\"meta_Grocery_and_Gourmet_Food.json.gz\",\"Grocery_and_Gourmet_Food.json.gz\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-l3awNsfWFL"
      },
      "source": [
        "#Cleaning\n",
        "def data_cleaning(df_merged,file_name,vid=False):\n",
        "  #Drop columns\n",
        "  df_merged=df_merged.drop(columns=['reviewTime','overall','category','reviewerID', 'style','reviewerName', 'unixReviewTime','image','summary','verified','vote','image', 'tech1','description', 'fit','tech2', 'rank', 'details', 'main_cat', 'date', 'price', 'imageURL', 'imageURLHighRes','also_buy','feature','also_view','similar_item'])\n",
        "\n",
        "  #Remove Duplicates\n",
        "  df_merged=df_merged.dropna(how='any',axis=0) \n",
        "\n",
        "  #Replace empty cell with Nan, Then drop all Nan\n",
        "  #Do this for 'brand' and 'title' and 'reviewText'\n",
        "  df_merged['brand'].replace('', np.nan, inplace=True)\n",
        "  df_merged.dropna(subset=['brand'], inplace=True)\n",
        "\n",
        "  df_merged['title'].replace('', np.nan, inplace=True)\n",
        "  df_merged.dropna(subset=['title'], inplace=True)\n",
        "\n",
        "  df_merged['reviewText'].replace('', np.nan, inplace=True)\n",
        "  df_merged.dropna(subset=['reviewText'], inplace=True)\n",
        "\n",
        "  #Remove duplicate reviews\n",
        "  df_merged=df_merged.drop_duplicates(subset=['reviewText'])\n",
        "\n",
        "  #For video category\n",
        "  if (vid==True):\n",
        "      df_merged['brand']=df_merged['brand'].str.replace(r'by\\n    \\n    ','')\n",
        "\n",
        "  #write to csv\n",
        "  #Also save to your drive for later analysis\n",
        "  df_merged.to_csv(file_name+'.csv')\n",
        "  return df_merged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md0PCekvuEBL"
      },
      "source": [
        "df_Arts=data_cleaning(df_Arts,'Arts_merged_noDuplicate',vid=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiqK5wTkfYP9"
      },
      "source": [
        "df_Video_Games=data_cleaning(df_Video_Games,'Video_Games_merged_noDuplicate',vid=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sSbDQPHuEDM"
      },
      "source": [
        "df_Grocery=data_cleaning(df_Grocery,'Grocery_merged_noDuplicate',vid=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8QsNuu5u09W"
      },
      "source": [
        "## Select the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Us7NJZjy0JW"
      },
      "source": [
        "def select_products(df, column, value, file_name):\n",
        "\n",
        "  final_df =df.loc[df[column] == value]\n",
        "  final_df=final_df.reset_index()\n",
        "  final_df.insert(0, 'ID', final_df.index + 1)\n",
        "  final_df= final_df.drop(columns=['index'])\n",
        "  final_df.to_csv(file_name+'.csv')\n",
        "  return final_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_9Enyzu3_D"
      },
      "source": [
        "#Select Art subsets\n",
        "#Brother Sewing Machine\n",
        "df_Art_Product_A=select_products(df_Video_Games,'asin','B000F7DPEQ','Art_Product_A')\n",
        "\n",
        "#SINGER Sewing Machine\n",
        "df_Art_Product_B =select_products(df_Video_Games,'asin','B005HR1JMO','Art_Product_B')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI8XbZzDu9E5"
      },
      "source": [
        "#Select Video Games subsets\n",
        "\n",
        "#Redragon Mouse\n",
        "df_VideoGame_Product_A=select_products(df_Video_Games,'asin','B00HTK1NCS','Games_Product_A')\n",
        "\n",
        "#Havit Mouse\n",
        "df_VideoGame_Product_B =select_products(df_Video_Games,'asin','B00KKAQYXM','Games_Product_B')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Lvw1qNu4B0"
      },
      "source": [
        "#Select Grocery subsets\n",
        "\n",
        "#Finding all coconut oil products\n",
        "df_oil_products=df_Grocery[df_Grocery['title'].str.contains(\"Coconut Oil\",na=False, case=False)]\n",
        "\n",
        "# we look at the top 20 products and manually check them and find which are sustainable and which are not.\n",
        "# to check https://www.amazon.com/dp/B00DS842HS\n",
        "df_oil_products.groupby('asin').count().sort_values('brand',ascending=False).head(20)\n",
        "\n",
        "asin_list_not_sustainable=['B00CPZPYLS','B008MOMYRI','B00HNTPEWU','B00HNTPF7E','B01D19RF3Q','B00M8HPGMU','B01DKX9CIE','B00US7LXUQ','B00A2A88ZW']\n",
        "asin_list_sustainable=['B00DS842HS','B000H2XXRS','B003OGKCDC','B000GAT6NG','B00Y8HZS1W','B00KRFLDBS','B002VLZ8D0','B001E8DHPW','B000R7YVF6','B0013OX8II']\n",
        "\n",
        "df_not_sustainable_oil=df_oil_products.loc[df_oil_products['asin'].isin(asin_list_not_sustainable)].sort_values('brand',ascending=False)\n",
        "df_sustainable_oil=df_oil_products.loc[df_oil_products['asin'].isin(asin_list_sustainable)].sort_values('brand',ascending=False)#\n",
        "\n",
        "df_not_sustainable_oil.to_csv('Grocery_not_sustainable_oil.csv')\n",
        "df_sustainable_oil.to_csv('Grocery_sustainable_oil.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTic1z4qfakJ"
      },
      "source": [
        "# Pre-Processsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA6wStjDI9gK"
      },
      "source": [
        "##Define finctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZnqePez77w8",
        "outputId": "48410577-c060-44b4-b6af-c621b6e660ae"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models   # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk; nltk.download('stopwords')\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gku3CaH7fetM"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgwdpCzD8L_d"
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc),min_len=3) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts, bigram_mod):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts,bigram_mod, trigram_mod):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hd0ZUhQ7it3"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "def build_Bi_Tri_models(data_words):\n",
        "  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "  trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "  \n",
        "  # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "  return bigram_mod, trigram_mod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-0Q3SORfeou"
      },
      "source": [
        "def preprocess(df,isTrigram,extra_stopwords_list):\n",
        "  data = df.reviewText.values.tolist()\n",
        "  # Remove Emails\n",
        "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "  # Remove new line characters\n",
        "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "  # Remove distracting single quotes\n",
        "  data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "  #Convert to words\n",
        "  data_words= list(sent_to_words(data))\n",
        "\n",
        "  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases. \n",
        "  # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "  # Remove Stop Words\n",
        "  stop_words.extend(extra_stopwords_list)\n",
        "  data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "  # Form Bigrams\n",
        "  data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
        "  \n",
        "  # Do lemmatization keeping only noun, adj, vb, adv\n",
        "  data_lemmatized_Bi = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "  return_data_processed= data_lemmatized_Bi\n",
        "  \n",
        "  if (isTrigram==True):\n",
        "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "    ## Form Trigrams\n",
        "    data_words_bigrams_trigrams = make_trigrams(data_words_bigrams, bigram_mod,trigram_mod)\n",
        "    data_lemmatized_Tri = lemmatization(data_words_bigrams_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "    return_data_processed = data_lemmatized_Tri\n",
        "\n",
        "\n",
        "  return return_data_processed ,data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjhje9108MEQ"
      },
      "source": [
        "#Creat Corpus and id2word\n",
        "def creat_corpus(data_processed,filter_extremes):\n",
        "  \n",
        "  id2word = corpora.Dictionary(data_processed)\n",
        "  id2word.filter_extremes(no_below=filter_extremes, no_above=0.5, keep_n=100000)\n",
        "  # Create Corpus\n",
        "  texts = data_processed\n",
        "  # Term Document Frequency\n",
        "  corpus = [id2word.doc2bow(text) for text in texts]\n",
        "  \n",
        "\n",
        "  return id2word, corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WUVOqRgIgzP"
      },
      "source": [
        "##Preprocess each subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDr7Mw3J8IL"
      },
      "source": [
        "#Arts\n",
        "\n",
        "#Custom stopwords\n",
        "extra_stopwords_list=['brother', 'sew', 'machine']\n",
        "\n",
        "#Borther Sewing Machine subset\n",
        "data_processed_Art_A, data_Art_A= preprocess(df=df_Art_Product_A,isTrigram=False,extra_stopwords_list=extra_stopwords_list)\n",
        "id2word_Art_A, corpus_Art_A = creat_corpus(data_processed,5)\n",
        "\n",
        "extra_stopwords_list=['Singer', 'sew', 'machine']\n",
        "#SINGER Sewing Machine subset\n",
        "data_processed_Art_B, data_Art_B= preprocess(df=df_Art_Product_B,isTrigram=True,extra_stopwords_list=extra_stopwords_list)\n",
        "id2word_Art_B, corpus_Art_B = creat_corpus(data_processed,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gMxLS6YDPsq"
      },
      "source": [
        "#Video Games\n",
        "\n",
        "#Custom stopwords\n",
        "extra_stopwords_list=['use','great','good','get','try','nice']\n",
        "\n",
        "#Redragin Gaming Mouse subset\n",
        "data_processed_VideoGames_A, data_VideoGames_A = preprocess(df=df_VideoGame_Product_A,isTrigram=True,extra_stopwords_list=extra_stopwords_list)\n",
        "id2word_VideoGames_A, corpus_VideoGames_A = creat_corpus(data_processed,10)\n",
        "\n",
        "#Havit Gaming Mouse subset\n",
        "data_processed_VideoGames_B, data_VideoGames_B= preprocess(df=df_VideoGame_Product_B,isTrigram=False,extra_stopwords_list=extra_stopwords_list)\n",
        "id2word_VideoGames_B, corpus_VideoGames_B = creat_corpus(data_processed,10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCta0huP8MGf"
      },
      "source": [
        "#Grocery\n",
        "\n",
        "#Custom stopwords\n",
        "extra_stopwords_list=['oil', 'product', 'coconut', 'great', 'love', 'good']\n",
        "\n",
        "#Sustainable coconut oils subset\n",
        "data_processed_Grocery_A, data_Grocery_A= preprocess(df=df_sustainable_oil,isTrigram=True,extra_stopwords_list=extra_stopwords_list)\n",
        "id2word_Grocery_A, corpus_Grocery_A = creat_corpus(data_processed,15)\n",
        "\n",
        "extra_stopwords_list=['oil', 'product', 'coconut', 'great', 'love', 'good', 'use']\n",
        "#Unsustainable coconut oils subset\n",
        "data_processed_Grocery_B,data_Grocery_B= preprocess(df=df_not_sustainable_oil,isTrigram=False,extra_stopwords_list=extra_stopwords_list)\n",
        "id2word_Grocery_B, corpus_Grocery_B = creat_corpus(data_processed,15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-RNxRhCffEo"
      },
      "source": [
        "# Topic Modeling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOXM_fzMp7f"
      },
      "source": [
        "##Build LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH8PBFqomouE"
      },
      "source": [
        "def compute_coherence_values( dictionary, corpus, texts, limit, chunksize=2000, passes=20, start=2, step=1 ):\n",
        "    \n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "\n",
        "    log_perplexity_values=[]\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=num_topics, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=chunksize,\n",
        "                                           passes=passes,\n",
        "                                           alpha='auto',\n",
        "                                           eta='auto',\n",
        "                                           per_word_topics=True)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        \n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYYaHbJqTLd0"
      },
      "source": [
        "#Plot graphs and scores\n",
        "def show_graph(coherence_values, title_1, limit=15,start=2, step=1):\n",
        "  #limit=15;start=2; step=1;\n",
        "  x = range(start, limit, step)\n",
        "  plt.plot(x, coherence_values, label='Coherence score')\n",
        "  plt.xlabel(\"Topics\")\n",
        "  plt.ylabel(\"Coherence score\")\n",
        "  plt.title(title_1)\n",
        "  plt.show()  \n",
        "\n",
        "  for m, cv in zip(x, coherence_values ):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
        "  print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-g_cw4IVuns"
      },
      "source": [
        "##The optimal model settings for each subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXLisK40XgUZ"
      },
      "source": [
        "In this section, we only run the below cells multiple times to find the best expriment. but for illustration, we set one run with best parameters and settings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jdmgmyob7SF"
      },
      "source": [
        "####Arts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmBFDkAwmowr"
      },
      "source": [
        "#Brother\n",
        "# may take long time to finish running\n",
        "model_list_prod_A, coherence_values_prod_A = compute_coherence_values(dictionary=id2word_Art_A, corpus=corpus_Art_A, texts=data_processed_Art_A,chunksize=1000,passes=20, start=2, limit=10, step=1)\n",
        "\n",
        "# Plot the results\n",
        "title_1='Coherence scores for Brother Sewing Machine models (Bigram)'\n",
        "show_graph(coherence_values_prod_A, title_1, limit=10,start=2, step=1)\n",
        "\n",
        "# Choose the optimal model based on the coherence and print top 15 words\n",
        "optimal_model_Art_A = model_list_prod_A[5]\n",
        "pprint(optimal_model_Art_A.print_topics(num_words=15))\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(optimal_model_Art_A, corpus_Art_A, id2word_Art_A)\n",
        "vis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoFB7UuQbLFM"
      },
      "source": [
        "#Brother\n",
        "# may take long time to finish running\n",
        "model_list_prod_B, coherence_values_prod_B = compute_coherence_values(dictionary=id2word_Art_B, corpus=corpus_Art_B, texts=data_processed_Art_B,chunksize=2000,passes=20, start=2, limit=10, step=1)\n",
        "\n",
        "# Plot the results\n",
        "title_1='Coherence scores for SINGER Sewing Machine models (Trigram)'\n",
        "show_graph(coherence_values_prod_B, title_1, limit=10,start=2, step=1)\n",
        "\n",
        "# Choose the optimal model based on the coherence and print top 15 words\n",
        "optimal_model_Art_B = model_list_prod_B[3]\n",
        "pprint(optimal_model_Art_B.print_topics(num_words=15))\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(optimal_model_Art_B, corpus_Art_B, id2word_Art_B)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mquY-Y-lb-rd"
      },
      "source": [
        "####Video Games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4lqMr1WV7SD"
      },
      "source": [
        "#Redragon Gaming Mouse\n",
        "# may take long time to finish running\n",
        "model_list_prod_A, coherence_values_prod_A = compute_coherence_values(dictionary=id2word_VideoGames_A, corpus=corpus_VideoGames_A, texts=data_processed_VideoGames_A,chunksize=2000,passes=20, start=2, limit=15, step=1)\n",
        "\n",
        "# Plot the results\n",
        "title_1='Coherence scores for Redragon Gaming Mouse models (Trigram)'\n",
        "show_graph(coherence_values_prod_A, title_1, limit=15,start=2, step=1)\n",
        "\n",
        "# Choose the optimal model based on the coherence and print top 15 words\n",
        "optimal_model_VideoGames_A = model_list_prod_A[6]\n",
        "pprint(optimal_model_VideoGames_A.print_topics(num_words=15))\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(optimal_model_VideoGames_A, corpus_VideoGames_A, id2word_VideoGames_A)\n",
        "vis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bAZmU2-bDYW"
      },
      "source": [
        "#Habvit Gaming Mouse\n",
        "# may take long time to finish running\n",
        "model_list_prod_B, coherence_values_prod_B = compute_coherence_values(dictionary=id2word_VideoGames_B, corpus=corpus_VideoGames_B, texts=data_processed_VideoGames_B,chunksize=2000,passes=20, start=2, limit=15, step=1)\n",
        "\n",
        "# Plot the results\n",
        "title_1='Coherence scores for Habvit Gaming Mouse models (Bigram)'\n",
        "show_graph(coherence_values_prod_B, title_1, limit=15,start=2, step=1)\n",
        "\n",
        "# Choose the optimal model based on the coherence and print top 15 words\n",
        "optimal_model_VideoGames_B = model_list_prod_B[3]\n",
        "pprint(optimal_model_VideoGames_B.print_topics(num_words=15))\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(optimal_model_VideoGames_B, corpus_VideoGames_B, id2word_VideoGames_B)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMIzwJ1cCXX"
      },
      "source": [
        "####Grocery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wkRrBtKTLae"
      },
      "source": [
        "#Sustainable coconut oils\n",
        "# may take long time to finish running\n",
        "model_list_prod_A, coherence_values_prod_A = compute_coherence_values(dictionary=id2word_Grocery_A, corpus=corpus_Grocery_A, texts=data_processed_Grocery_A,chunksize=20000,passes=30, start=2, limit=30, step=1)\n",
        "\n",
        "# Plot the results\n",
        "title_1='Coherence scores for unsustainable products models (Trigram)'\n",
        "show_graph(coherence_values_prod_A, title_1, limit=30,start=2, step=1)\n",
        "\n",
        "# Choose the optimal model based on the coherence and print top 15 words\n",
        "optimal_model_Grocery_A = model_list_prod_A[7]\n",
        "pprint(optimal_model_Grocery_A.print_topics(num_words=15))\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(optimal_model_Grocery_A, corpus_Grocery_A, id2word_Grocery_A)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APaNzhOYcc2l"
      },
      "source": [
        "#Unsustainable coconut oils\n",
        "# may take long time to finish running\n",
        "model_list_prod_B, coherence_values_prod_B = compute_coherence_values(dictionary=id2word_Grocery_B, corpus=corpus_Grocery_B, texts=data_processed_Grocery_B,chunksize=10000,passes=20, start=2, limit=25, step=1)\n",
        "\n",
        "# Plot the results\n",
        "title_1='Coherence scores for unsustainable products models (Bigram)'\n",
        "show_graph(coherence_values_prod_B, title_1, limit=25,start=2, step=1)\n",
        "\n",
        "# Choose the optimal model based on the coherence and print top 15 words\n",
        "optimal_model_Grocery_B = model_list_prod_B[8]\n",
        "pprint(optimal_model_Grocery_B.print_topics(num_words=15))\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(optimal_model_Grocery_B, corpus_Grocery_B, id2word_Grocery_B)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QI_TgflmozE"
      },
      "source": [
        "#Save the model\n",
        "#optimal_model.save('model_name.model')\n",
        "\n",
        "#Load if there is saved models\n",
        "#optimal_model = gensim.models.ldamodel.LdaModel.load('model_name.model')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWawn_1FY6p1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKNiD6xRfXFG"
      },
      "source": [
        "##Find the dominant topic in each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUlJ3vv7fp5h"
      },
      "source": [
        "def find_dominant_topic(ldamodel, corpus, texts):\n",
        "  # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "      row = sorted(row[0], key=lambda x: (x[1]), reverse=True) # This for gensim lda model\n",
        "      # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "      for j, (topic_num, prop_topic) in enumerate(row):\n",
        "          if j == 0: # => dominant topic\n",
        "              wp = ldamodel.show_topic(topic_num)\n",
        "              topic_keywords = \", \".join([word for word, prop in wp])             \n",
        "              sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "          else:\n",
        "              break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original review to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcGBhOWgkPI"
      },
      "source": [
        "###Arts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GEDFth9fp8k"
      },
      "source": [
        "#Brother sewing machine\n",
        "df_topic_keywords = find_dominant_topic(ldamodel=optimal_model_Art_A, corpus=corpus_Art_A, texts=data_Art_A)\n",
        "\n",
        "# Format\n",
        "df_review_topic_Art_A = df_topic_keywords\n",
        "df_review_topic_Art_A.columns = [ 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "#SINGER sewing machine\n",
        "df_topic_keywords = find_dominant_topic(ldamodel=optimal_model_Art_B, corpus=corpus_Art_B, texts=data_Art_B)\n",
        "\n",
        "# Format\n",
        "df_review_topic_Art_B = df_topic_keywords\n",
        "df_review_topic_Art_B.columns = [ 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7rytcl2gnEV"
      },
      "source": [
        "###Video Games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B6DSHZlgwXf"
      },
      "source": [
        "#Redragon gaming mouse\n",
        "df_topic_keywords = find_dominant_topic(ldamodel=optimal_model_VideoGames_A, corpus=corpus_VideoGames_A, texts=data_VideoGames_A)\n",
        "\n",
        "# Format\n",
        "df_review_topic_VideoGames_A= df_topic_keywords\n",
        "df_review_topic_VideoGames_A.columns = [ 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "#Havit gaming mouse\n",
        "df_topic_keywords = find_dominant_topic(ldamodel=optimal_model_VideoGames_B, corpus=corpus_VideoGames_B, texts=data_VideoGames_B)\n",
        "\n",
        "# Format\n",
        "df_review_topic_VideoGames_B = df_topic_keywords\n",
        "df_review_topic_VideoGames_B.columns = [ 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6r0MguignKK"
      },
      "source": [
        "###Grocery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghvbk2ybfp-k"
      },
      "source": [
        "#Sustainable coconut oils\n",
        "df_topic_keywords = find_dominant_topic(ldamodel=optimal_model_Grocery_A, corpus=corpus_Grocery_A, texts=data_Grocery_A)\n",
        "\n",
        "# Format\n",
        "df_review_topic_Grocery_A= df_topic_keywords\n",
        "df_review_topic_Grocery_A.columns = [ 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "#Unsustainable coconut oils\n",
        "df_topic_keywords = find_dominant_topic(ldamodel=optimal_model_Grocery_B, corpus=corpus_Grocery_B, texts=data_Grocery_B)\n",
        "\n",
        "# Format\n",
        "df_review_topic_Grocery_B = df_topic_keywords\n",
        "df_review_topic_Grocery_B.columns = [ 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKPmef0mpa4"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5CxzQnVmy0i",
        "outputId": "a8e07503-82ac-4d42-a69f-b087720ac4d5"
      },
      "source": [
        "import nltk\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb4BfkPQmy7O"
      },
      "source": [
        "# Calculate sentiment score \n",
        "def sentimentScore(df):\n",
        "    #get reviews only\n",
        "    sentences = df['Text']\n",
        "    #initialise the vader object\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    results = []\n",
        "    #pass each review to the function\n",
        "    for sentence in sentences:\n",
        "        vs = analyzer.polarity_scores(sentence)\n",
        "        #print(str(vs))\n",
        "        results.append(vs)\n",
        "\n",
        "    #concat the sentiment score to the original df\n",
        "    sentiment_df = pd.DataFrame(results) #convert list to df\n",
        "    df.index = sentiment_df.index #match index\n",
        "    sentiment_and_Topics = pd.concat([df, sentiment_df], axis=1) #concat both df \n",
        "\n",
        "    ## Convert sentiment score to text\n",
        "    compound = sentiment_and_Topics['compound']\n",
        "    sentiment=[]\n",
        "    for i in range(len(compound)):\n",
        "      if compound[i] >= 0.05 :\n",
        "            sentiment.append(\"Positive\")\n",
        "    \n",
        "      elif compound[i] <= - 0.05 :\n",
        "            sentiment.append(\"Negative\")\n",
        "    \n",
        "      else :\n",
        "            sentiment.append(\"Neutral\")\n",
        "\n",
        "    ## produce the final DF\n",
        "    overall_sentiment=pd.DataFrame(sentiment)\n",
        "    sentiment_and_Topics = pd.concat([sentiment_and_Topics, overall_sentiment], axis=1)\n",
        "    sentiment_and_Topics.columns = [*sentiment_and_Topics.columns[:-1], 'overall_sentiment']\n",
        "    \n",
        "    \n",
        "    return sentiment_and_Topics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbwtCNupwhW"
      },
      "source": [
        "#Sentiment for Art products\n",
        "df_final_Art_Product_A= sentimentScore(df_review_topic_Art_A)\n",
        "df_final_Art_Product_B= sentimentScore(df_review_topic_Art_B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-50iT2Gol81"
      },
      "source": [
        "#Sentiment for Video Games products\n",
        "df_final_VideoGame_Product_A= sentimentScore(df_review_topic_VideoGames_A)\n",
        "df_final_VideoGame_Product_B= sentimentScore(df_review_topic_VideoGames_B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUZZfVOqjiFF"
      },
      "source": [
        "#Sentiment for Grocery products\n",
        "df_final_Grocery_Group_A= sentimentScore(df_sustainable_oil)\n",
        "df_final_Grocery_Group_B= sentimentScore(df_not_sustainable_oil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mcXOT1IvRWf"
      },
      "source": [
        "#Save for analysis and visulization\n",
        "#Product_with_sentiment_and_Topics.to_csv('Product_with_sentiment_and_Topics.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydKNzIGemzaL"
      },
      "source": [
        "# Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlMugs4Km38Z"
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.colors as mcolors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fY_G3dPvLML"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqAxKenDeT5a"
      },
      "source": [
        "def draw_overall_pie_2(df, topics_list,title):\n",
        "  \n",
        "  y = df.Total.to_list()\n",
        "  colors = ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan','darkolivegreen','tan','darkmagenta']\n",
        "\n",
        "  patches, texts = plt.pie(y, startangle=90, radius=1.2,colors=colors)\n",
        "  labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(topics_list, df.Percentage.to_list())]\n",
        "\n",
        "  sort_legend = True\n",
        "  if sort_legend:\n",
        "      patches, labels, dummy =  zip(*sorted(zip(patches, labels, y),\n",
        "                                            key=lambda x: x[2],\n",
        "                                            reverse=True))\n",
        "\n",
        "  plt.legend(patches, labels, loc='left center', bbox_to_anchor=(-0.1, 1.),\n",
        "            fontsize=12)\n",
        "  plt.xlabel(title,labelpad=20,fontsize=14)\n",
        "\n",
        "#plt.savefig('piechart.png', bbox_inches='tight')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7cZfodFhu2"
      },
      "source": [
        "def draw_overall_pie(df,topics_list,title):\n",
        "  \n",
        "  values = df.Total.to_list()\n",
        "\n",
        "  my_labels = topics_list\n",
        "  pie=plt.pie(values,autopct='%1.2f%%',textprops={'fontsize': 14})\n",
        "  #plt.title(\"Brother Sewing Machine Topics (Arts Crafts)\",fontsize=14, y=1.08)\n",
        "  plt.xlabel(title,labelpad=20,fontsize=14)\n",
        "  plt.legend(pie[0],my_labels, bbox_to_anchor=(.90,0.5), loc=\"center right\", fontsize=10, \n",
        "            bbox_transform=plt.gcf().transFigure)\n",
        "  plt.subplots_adjust(left=0.0, bottom=0.1, right=0.50)\n",
        "\n",
        "  plt.axis('equal')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oMtTB8wvXzw"
      },
      "source": [
        "\n",
        "def draw_pie(df, topic_id):\n",
        "  \n",
        "  title = '\"'+df.index[topic_id] +'\" with '+ str(df['Total'].iloc[topic_id])+' Reviews'\n",
        "  values = df.iloc[topic_id, 0:3].to_list()\n",
        "\n",
        "  my_labels = 'Positive','Neutral','Negative'\n",
        "  plt.pie(values,labels=my_labels,autopct='%1.2f%%',textprops={'fontsize': 14})\n",
        "  plt.title(title,fontsize=14 )\n",
        "  plt.axis('equal')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZNWU1mPjztH"
      },
      "source": [
        "def creat_df(df ,topics_list ):\n",
        "\n",
        "  N_topics = df['Dominant_Topic'].unique()\n",
        "  N_topics=sorted(N_topics)\n",
        "\n",
        "  positive= df[df['overall_sentiment']=='Positive'].groupby('Dominant_Topic').count()['Text']\n",
        "  Neutral= df[df['overall_sentiment']=='Neutral'].groupby('Dominant_Topic').count()['Text']\n",
        "  Negative= df[df['overall_sentiment']=='Negative'].groupby('Dominant_Topic').count()['Text']\n",
        "  positive=list(positive)\n",
        "  Neutral=list(Neutral)\n",
        "  Negative=list(Negative)\n",
        "  \n",
        "  df = pd.DataFrame(positive, columns=['Positive'], index=topics_list)\n",
        "  #df['Positive'] = positive\n",
        "  df['Neutral'] = Neutral\n",
        "  df['Negative'] = Negative\n",
        "  df['Total'] = df.sum(numeric_only=True,axis=1)\n",
        "  df['Percentage'] = df.Total / df.Total.sum() *100\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOEtL-6Xotyv"
      },
      "source": [
        "def show_perc_graph(df , title ,topics_list):\n",
        "\n",
        "\n",
        "  df_1= creat_df(df,topics_list)\n",
        "  print(df_1)\n",
        "  \n",
        "  #pie for sentiment in each topic\n",
        "  for i in range(0,len(topics_list)):\n",
        "    draw_pie(df_1,i)\n",
        "  #pie of all topics\n",
        "  draw_overall_pie(df_1,topics_list,title)\n",
        "  #pie of all topics version 2\n",
        "  draw_overall_pie_2(df_1,topics_list,title)\n",
        "\n",
        "  \n",
        "  fig,ax= plt.subplots()\n",
        "  df_1[['Positive','Neutral','Negative']].plot(\n",
        "  #x = index, \n",
        "  ax=ax,\n",
        "  kind = 'barh', \n",
        "  stacked = True, \n",
        "  figsize=(15,7)\n",
        "  #title = \"Count of CR over Topics for \"+title)\n",
        "  )\n",
        "  \n",
        "  plt.xlabel('Count of CR',fontsize=13)\n",
        "  plt.ylabel(\"Topics\",fontsize=13)\n",
        "  plt.title(\"Count of CR over Topics for \"+title, fontsize=15)\n",
        "  plt.yticks(fontsize=14)\n",
        "  plt.xticks(fontsize=10)\n",
        "  #Label the bars\n",
        "  for i, v in enumerate(df_1.Total):\n",
        "    plt.text(v+0.2, i, str(round(v, 2)), va=\"center\", fontsize=15)\n",
        "  \n",
        "  # use axvline to draw the mean of reviews\n",
        "  mean = df_1[\"Total\"].mean()\n",
        "  ax.axvline(mean,color='red',linewidth=1,ls='--',label='Î¼='+str(round(mean)))\n",
        "  plt.legend(fontsize=14)\n",
        " \n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swgtuAUsvLO2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf-I7iE49eAv"
      },
      "source": [
        "##Art"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aAcP1GAf07x"
      },
      "source": [
        "topics_list=['Multiuse','Functionality','Price','Clarity of\\n Instructions','Ease of Use','Customer Service','Thread Tension']\n",
        "show_perc_graph(df_final_Art_Product_A, 'Brother Sewing Machine (Arts and Crafts)',topics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twrntkazf0-u"
      },
      "source": [
        "topics_list=['Price','Thread Tension','Functionality','Customer Service','Buy as Gift','Ease of Use']\n",
        "show_perc_graph(df_final_Art_Product_B, 'SINGER Sewing Machine (Arts and Crafts)',topics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivX3nitQ9cw6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyg0i9OP9gZJ"
      },
      "source": [
        "##Video Games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My1XCloEl_rm"
      },
      "source": [
        "topics_list=['DPI Settings','Mouse Buttons','Quality','Backlit Feature','Software Compatibility','Ergonomic Design','Price','Product Lifespan']\n",
        "show_perc_graph(df_final_VideoGame_Product_A, 'Redragon Gaming Mouse (Video Games)',topics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLsrQzVCmAL6"
      },
      "source": [
        "topics_list=['Scroll Wheel','Colour Change','Mouse Buttons','Cord','Ergonomic Design']\n",
        "show_perc_graph(df_final_VideoGame_Product_B, 'Havit Gaming Mouse (Video Games)',topics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_FBULvMvLQ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2f4Z2Fb9igD"
      },
      "source": [
        "##Grocery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eiq-eYEkMVuq"
      },
      "source": [
        "topics_list=['Packaging','Customers experiences\\nwith various brands','Texture','Price and Quality','Benefit for Skin','Smell','Flavour and Cooking','Function as Hair Product','Delivery Experience']\n",
        "show_perc_graph(df_final_Grocery_Group_A, 'Sustainable Coconut Oils Products\\n(Grocery and Gourmet Food)',topics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDfoeLjdLOF-"
      },
      "source": [
        "topics_list=['Packaging','Used with Popcorn','Time Frame','Used for Cooking','Falvour','Versatility','Added to Coffee','Used for Beauty','Price and Quality','Health Benefits','Smell','Delivery Experience','Recommend the Product']\n",
        "show_perc_graph(df_final_Grocery_Group_B, 'Unsustainable Coconut Oils Products\\n(Grocery and Gourmet Food)',topics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adj-YPeIvLTf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PxWvoDA-gud"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YvPPL6Q-kyC"
      },
      "source": [
        "# Parts of this code has been modified from machinelearningplus.com, the liscence below:\n",
        "\n",
        "# All Rights Reserved. This notebook is proprietary content of machinelearningplus.com. \n",
        "# This can be shared solely for educational purposes, with due credits to machinelearningplus.com\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
